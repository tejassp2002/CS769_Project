\section{Experiments}
\label{sec:experiments}
We focus on routing problems: we consider the TSP, two variants of the VRP, the Orienteering Problem and the (Stochastic) Prize Collecting TSP. These provide a range of different challenges, constraints and objectives and are \emph{traditionally solved by different algorithms}. For the Attention Model (AM), we adjust the input, mask, decoder context and objective function for each problem (see Appendix for details and data generation) and train on problem instances of $n=20$, 50 and 100 nodes. For all problems, we use \emph{the same hyperparameters}: those we found to work well on TSP.

\paragraph{Hyperparameters}
We initialize parameters $\text{Uniform}(-1/{\sqrt{d}}, 1/{\sqrt{d}})$, with $d$ the input dimension. Every epoch we process 2500 batches of 512 instances (except for VRP with $n=100$, where we use 2500 $\times$ 256 for memory constraints). For TSP, an epoch takes 5:30 minutes for $n=20$, 16:20 for $n=50$ (single GPU 1080Ti) and 27:30 for $n=100$ (on 2 1080Ti's). We train for 100 epochs using training data generated on the fly. We found training to be stable and results to be robust against different seeds, where only in one case (PCTSP with $n=20$) we had to restart training with a different seed because the run diverged. We use $N=3$ layers in the encoder, which we found is a good trade-off between quality of the results and computational complexity. We use a constant learning rate $\eta = 10^{-4}$. Training with a higher learning rate $\eta = 10^{-3}$ is possible and speeds up initial learning, but requires decay ($0.96$ per epoch) to converge and may be a bit more unstable. See Appendix \ref{sec:appendix_results_tsp}. With the rollout baseline, we use an exponential baseline ($\beta = 0.8$) during the first epoch, to stabilize initial learning, although in many cases learning also succeeds without this `warmup'. Our code in PyTorch \citep{paszke2017automatic} is publicly available.\footnote{\url{https://github.com/wouterkool/attention-learn-to-route}}

\paragraph{Decoding strategy and baselines}
For each problem, we report performance on 10000 test instances.
At test time we use \emph{greedy} decoding, where we select the best action (according to the model) at each step, or \emph{sampling}, where we sample 1280 solutions (in $<1$s on a single GPU) and report the best. More sampling improves solution quality at increased computation.
In Table \ref{tab:results_problems} we compare greedy decoding against baselines that also construct a single solution, and compare sampling against baselines that also consider multiple solutions, either via sampling or (local) search. For each problem, we also report the `best possible solution': either optimal via \citetalias{gurobi} (intractable for $n > 20$ except for TSP) or a problem specific state-of-the-art algorithm.

\paragraph{Run times}
Run times are important but hard to compare: they can vary by two orders of magnitude as a result of implementation (Python vs C++) and hardware (GPU vs CPU). We take a practical view and report the time it takes to solve the test set of 10000 instances, either on a single GPU (1080Ti) or 32 instances in parallel on a 32 virtual CPU system (2 $\times$ Xeon E5-2630). This is conservative: our model is parallelizable while most of the baselines are single thread CPU implementations which cannot parallelize when running individually. Also we note that after training our run time can likely be reduced by model compression \citep{hinton2015distilling}. In Table \ref{tab:results_problems} we do not report running times for the results which were reported by others as they are not directly comparable but we note that in general our model and implementation is fast: for instance \citet{bello2016neural} report 10.3s for sampling 1280 TSP solutions (K80 GPU) which we do in less than one second (on a 1080Ti). For most algorithms it is possible to trade off runtime for performance. As reporting full trade-off curves is impractical we tried to pick reasonable spots, reporting the fastest if results were similar or reporting results with different time limits (for example we use Gurobi with time limits as heuristic).

\input{./tables/results_problems.tex}
\subsection{Problems}

\paragraph{Travelling Salesman Problem (TSP)}
For the TSP, we report optimal results by Gurobi, as well as by Concorde \citep{concorde} (faster than Gurobi as it is specialized for TSP) and LKH3 \citep{helsgaun2017extension}, a state-of-the-art heuristic solver that empirically also finds optimal solutions in time comparable to Gurobi. We compare against Nearest, Random and Farthest Insertion, as well as Nearest Neighbor, which is the only non-learned baseline algorithm that also constructs a tour directly in order (i.e. is structurally similar to our model). For details, see Appendix \ref{sec:appendix_baselines}. Additionally we compare against the learned heuristics in Section \ref{sec:related_work}, most importantly \citet{bello2016neural}, as well as OR Tools reported by \citet{bello2016neural} and Christofides + 2OPT local search reported by \citet{vinyals2015pointer}. Results for \citet{dai2017learning} are (optimistically) computed from the \emph{optimality gaps} they report on 15-20, 40-50 and 50-100 node graphs, respectively. Using a \emph{single} greedy construction we outperform traditional baselines and we are able to achieve significantly closer to optimal results than previous learned heuristics (from around 1.5\% to 0.3\% above optimal for $n=20$). Naturally, the difference with \citet{bello2016neural} gets diluted when sampling many solutions (as with many samples even a random policy performs well), but we still obtain significantly better results, \emph{without} tuning the softmax temperature. For completeness, we also report results from running the Encode-Attend-Navigate (EAN) code\footnote{\url{https://github.com/MichelDeudon/encode-attend-navigate}} which is concurrent work by \citet{deudon2018learning} (for details see Appendix \ref{app:deudon}). Our model outperforms EAN, even if EAN is improved with 2OPT local search. Appendix \ref{sec:appendix_results_tsp} presents the results visually, including generalization results for different $n$.

\paragraph{Vehicle Routing Problem (VRP)}
In the Capacitated VRP (CVRP) \citep{toth2014vehicle}, each node has a demand and multiple routes should be constructed (starting and ending at the depot), such that the total demand of the nodes in each route does not exceed the vehicle capacity. We also consider the Split Delivery VRP (SDVRP), which allows to split customer demands over multiple routes. We implement the datasets described by \citet{nazari2018reinforcement} and compare against their Reinforcement Learning (RL) framework and the strongest baselines they report. Comparing greedy decoding, we obtain significantly better results. We cannot directly compare our sampling (1280 samples) to their beam search with size 10 (they do not report sampling or larger beam sizes), but note that our greedy method also outperforms their beam search in most (larger) cases, getting (in $<$1 second/instance) much closer to LKH3 \citep{helsgaun2017extension}, a state-of-the-art algorithm which found best known solutions to CVRP benchmarks. See Appendix \ref{sec:appendix_vrp_examples} for greedy example solution plots.

\paragraph{Orienteering Problem (OP)}
The OP \citep{golden1987orienteering} is an important problem used to model many real world problems. Each node has an associated \emph{prize}, and the goal is to construct a single tour (starting and ending at the depot) that \emph{maximizes} the sum of prizes of nodes visited while being shorter than a maximum (given) length. We consider the prize distributions proposed in \citet{fischetti1998solving}: \emph{constant}, \emph{uniform} (in Appendix \ref{sec:appendix_op_extended_results}), and increasing with the \emph{distance} to the depot, which we report here as this is the hardest problem. As `best possible solution' we report Gurobi (intractable for $n > 20$) and \emph{Compass}, the recent state-of-the-art Genetic Algorithm (GA) by \citet{kobeaga2018efficient}, which is only 2\% better than sampling 1280 solutions with our method (objective is maximization). We outperform a Python GA\footnote{\url{https://github.com/mc-ride/orienteering}} (which seems not to scale), as well the construction phase of the heuristic by \citet{tsiligirides1984heuristic} (comparing greedy or 1280 samples) which is structurally similar to the one learned by our model. OR Tools fails to find feasible solutions in a few percent of the cases for $n > 20$.

\paragraph{Prize Collecting TSP (PCTSP)}
In the PCTSP \citep{balas1989prize}, each node has not only an associated prize, but also an associated penalty. The goal is to collect at least a \emph{minimum} total prize, while minimizing the total tour length plus the sum of penalties of unvisited nodes. This problem is difficult as an algorithm has to trade off the penalty for not visiting a node with the marginal cost/tour length of visiting (which depends on the other nodes visited), while also satisfying the minimum total prize constraint. We compare against OR Tools with 10 or 60 seconds of local search, as well as open source C++\footnote{\url{https://github.com/jordanamecler/PCTSP}} and Python\footnote{\url{https://github.com/rafael2reis/salesman}} implementations of Iterated Local Search (ILS). Although the Attention Model does not find better solutions than OR Tools with 60s of local search, it finds almost equally good results in significantly less time. The results are also within 2\% of the C++ ILS algorithm (but obtained much faster), which was the best open-source algorithm for PCTSP we could find.

\paragraph{Stochastic PCTSP (SPCTSP)}
The Stochastic variant of the PCTSP (SPCTSP) we consider shows how our model can deal with uncertainty naturally. In the SPCTSP, the \emph{expected} node prize is known upfront, but the real collected prize only becomes known upon visitation. With penalties, this problem is a generalization of the stochastic k-TSP \citep{ene2018approximation}. Since our model constructs a tour one node at the time, we only need to use the real prizes to compute the remaining prize constraint. By contrast, any algorithm that selects a fixed tour may fail to satisfy the prize constraint so an algorithm \emph{must} be adaptive. As a baseline, we implement an algorithm that plans a tour, executes part of it and then re-optimizes using the C++ ILS algorithm. We either execute \emph{all} node visits (so planning additional nodes if the result does not satisfy the prize constraint), \emph{half} of the planned node visits (for $O(\log n)$ replanning iterations) or only the \emph{first} node visit, for maximum adaptivity. We observe that our model outperforms all baselines for $n = 20$. We think that failure to account for uncertainty (by the baselines) in the prize might result in the need to visit one or two additional nodes, which is relatively costly for small instances but relatively cheap for larger $n$. Still, our method is beneficial as it provides competitive solutions at a fraction of the computational cost, which is important in online settings.

\subsection{Attention Model vs. Pointer Network and different baselines}


\begin{wrapfigure}{R}{0.45\textwidth}
\vspace{-0.25in}
\begin{center}
\centerline{\includegraphics[width=\linewidth]{./images/baselines}}
\caption{Held-out validation set optimality gap as a function of the number of epochs for the Attention Model (AM) and Pointer Network (PN) with different baselines (two different seeds).}
\label{fig:progress}
\end{center}
\vskip -0.25in
\end{wrapfigure}
Figure \ref{fig:progress} compares the performance of the TSP20 Attention Model (AM) and our implementation of the Pointer Network (PN) during training. We use a validation set of size 10000 with greedy decoding, and compare to using an exponential ($\beta = 0.8$) and a critic (see Appendix \ref{sec:appendix_tsp_critic}) baseline. We used two random seeds and a decaying learning rate of $\eta = 10^{-3} \times 0.96^{\text{epoch}}$. This performs best for the PN, while for the AM results are similar to using $\eta = 10^{-4}$ (see Appendix \ref{sec:appendix_results_tsp}). This clearly illustrates how the improvement we obtain is the result of both the AM and the rollout baseline: the AM outperforms the PN using any baseline and the rollout baseline improves the quality and convergence speed for both AM and PN. For the PN with critic baseline, we are unable to reproduce the $1.5\%$ reported by \citet{bello2016neural} (also when using an LSTM based critic), but our reproduction is closer than others have reported~\citep{dai2017learning,nazari2018reinforcement}. In Table \ref{tab:results_problems} we compare against the original results. Compared to the rollout baseline, the exponential baseline is around 20\% faster per epoch, whereas the critic baseline is around 13\% slower (see Appendix \ref{sec:appendix_results_tsp}), so the picture does not change significantly if time is used as x-axis.