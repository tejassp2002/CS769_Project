
\section{Related work}
\label{sec:related_work}
The application of Neural Networks (NNs) for optimizing decisions in combinatorial optimization problems dates back to \citet{hopfield1985neural}, who applied a Hopfield-network for solving small TSP instances. NNs have been applied to many related problems \citep{smith1999neural}, although in most cases in an \emph{online} manner, starting `from scratch' and `learning' a solution for every instance. More recently, (D)NNs have also been used \emph{offline} to learn about an entire class of problem instances.

\citet{vinyals2015pointer} introduce the Pointer Network (PN) as a model that uses attention to output a permutation of the input, and train this model offline to solve the (Euclidean) TSP, supervised by example solutions. Upon test time, their beam search procedure filters invalid tours. \citet{bello2016neural} introduce an Actor-Critic algorithm to train the PN without supervised solutions. They consider each instance as a training sample and use the cost (tour length) of a sampled solution for an unbiased Monte-Carlo estimate of the policy gradient. They introduce extra model depth in the decoder by an additional \emph{glimpse} \citep{vinyals2015order} at the embeddings, masking nodes already visited. For small instances ($n = 20$), they get close to the results by \mbox{\citet{vinyals2015pointer}}, they improve for $n=50$ and additionally include results for $n = 100$. \citet{nazari2018reinforcement} replace the LSTM encoder of the PN by element-wise projections, such that the updated embeddings after state-changes can be effectively computed. They apply this model on the Vehicle Routing Problem (VRP) with split deliveries and a stochastic variant.

\citet{dai2017learning} do not use a separate encoder and decoder, but a single model based on graph embeddings. They train the model to output the \emph{order} in which nodes are \emph{inserted} into a partial tour, using a helper function to insert at the best possible location. Their 1-step DQN \citep{mnih2015human} training method trains the algorithm per step and incremental rewards provided to the agent at every step effectively encourage greedy behavior. As mentioned in their appendix, they use the negative of the reward, which combined with discounting encourages the agent to insert the farthest nodes first, which is known to be an effective heuristic \citep{rosenkrantz2009analysis}.

\citet{nowak2017note} train a Graph Neural Network in a supervised manner to directly output a tour as an adjacency matrix, which is converted into a feasible solution by a beam search. The model is non-autoregressive, so cannot condition its output on the partial tour and the authors report an optimality gap of $2.7 \%$ for $n = 20$, worse than autoregressive approaches mentioned in this section. \citet{kaempfer2018learning} train a model based on the Transformer architecture \citep{vaswani2017attention} that outputs a fractional solution to the multiple TSP (mTSP). The result can be seen as a solution to the linear relaxation of the problem and they use a beam search to obtain a feasible integer solution. 

Independently of our work, \citet{deudon2018learning} presented a model for TSP using attention in the OR community. They show performance can improve using 2OPT local search, but do not show benefit of their model in direct comparison to the PN. We use a different decoder and improved training algorithm, both contributing to significantly improved results, \emph{without} 2OPT and additionally show application to different problems. For a full discussion of the differences, we refer to Appendix \ref{app:deudon}.