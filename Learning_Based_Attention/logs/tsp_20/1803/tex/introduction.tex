\section{Introduction}
Imagine yourself travelling to a scientific conference. The field is popular, and surely you do not want to miss out on anything. You have selected several posters you want to visit, and naturally you must return to the place where you are now: the coffee corner. In which order should you visit the posters, to minimize your time walking around? This is the Travelling Scientist Problem (TSP).

You realize that your problem is equivalent to the Travelling Salesman Problem (conveniently also TSP). This seems discouraging as you know the problem is (NP-)hard \citep{garey1979computers}. Fortunately, complexity theory analyzes the worst case, and your Bayesian view considers this unlikely. In particular, you have a strong prior: the posters will probably be laid out regularly. You want a special algorithm that solves not any, but \emph{this} type of problem instance. You have some months left to prepare. As a machine learner, you wonder whether your algorithm can be learned?

\paragraph{Motivation}
Machine learning algorithms have replaced humans as the engineers of algorithms to solve various tasks. A decade ago, computer vision algorithms used hand-crafted features but today they are learned \emph{end-to-end} by Deep Neural Networks (DNNs). DNNs have outperformed classic approaches in speech recognition, machine translation, image captioning and other problems, by learning from data~\citep{lecun2015deep}. While DNNs are mainly used to make \emph{predictions}, Reinforcement Learning (RL) has enabled algorithms to learn to make \emph{decisions}, either by interacting with an environment, e.g. to learn to play Atari games \citep{mnih2015human}, or by inducing knowledge through look-ahead search: this was used to master the game of Go \citep{silver2017mastering}.

The world is not a game, and we desire to train models that make decisions to solve real problems. These models must learn to select good solutions for a problem from a combinatorially large set of potential solutions. Classically, approaches to this problem of \emph{combinatorial optimization} can be divided into \emph{exact methods}, that guarantee finding optimal solutions, and \emph{heuristics}, that trade off optimality for computational cost, although exact methods can use heuristics internally and vice versa. Heuristics are typically expressed in the form of rules, which can be interpreted as policies to make decisions. We believe that these policies can be parameterized using DNNs, and be trained to obtain new and stronger algorithms for many different combinatorial optimization problems, similar to the way DNNs have boosted performance in the applications mentioned before.
In this paper, we focus on routing problems: an important class of practical combinatorial optimization problems.

The promising idea to learn heuristics has been tested on TSP \citep{bello2016neural}. In order to push this idea, we need better models and better ways of training. Therefore, we propose to use a powerful model based on attention and we propose to train this model using REINFORCE with a simple but effective greedy rollout baseline. The goal of our method is not to outperform a non-learned, specialized TSP algorithm such as Concorde \citep{concorde}. Rather, we show the flexibility of our approach on multiple (routing) problems of reasonable size, with \emph{a single set of hyperparameters}. This is important progress towards the situation where we can learn strong heuristics to solve a wide range of different practical problems for which no good heuristics exist. 